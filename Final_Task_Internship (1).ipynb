{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7fZ7LR4Ss9it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5d3c0c-440f-4740-f32d-fae2d3919557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Step 1 : Install Pandas\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # data visualization\n",
        "import seaborn as sns # statistical data visualization\n",
        "%matplotlib inline\n",
        "# import plotly.express as px\n",
        "# import plotly.graph_objects as go\n",
        "# import plotly.io as pio\n",
        "# import itertools\n",
        "\n"
      ],
      "metadata": {
        "id": "y3tWFIRERr2-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split , GridSearchCV, KFold, cross_val_score, RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay # Import ConfusionMatrixDisplay\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "_hTiDYIsSIey"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "G6bxNhP6ufTq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "MdfQFBz3r9_0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Understanding**"
      ],
      "metadata": {
        "id": "06MsUorjEUaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 : import data\n",
        "data_df = pd.read_csv('loan_data_2007_2014.csv')"
      ],
      "metadata": {
        "id": "XQtk80MAusg9",
        "outputId": "5cc56132-fdaa-44ae-b8be-3577cd0c2cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'loan_data_2007_2014.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b11e2ddd12d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 3 : import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loan_data_2007_2014.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'loan_data_2007_2014.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display first 5 rows\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "pe0De4fiu64E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.tail()"
      ],
      "metadata": {
        "id": "3AD0mcxiu_OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display info\n",
        "data_df.info()"
      ],
      "metadata": {
        "id": "PC199ZLVzIgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mengecek variabel yang masih terdapat nilai null\n",
        "data_df.isnull().sum()"
      ],
      "metadata": {
        "id": "33cn-Te_zM3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "nDxKfQd7MeeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.shape"
      ],
      "metadata": {
        "id": "Rv01zWUxFnWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head()"
      ],
      "metadata": {
        "id": "7VKeae_fsk6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = data_df.columns\n",
        "col_names"
      ],
      "metadata": {
        "id": "1BTBu7ujspXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find categorical variables\n",
        "\n",
        "categorical = [var for var in data_df.columns if data_df[var].dtype=='O']\n",
        "\n",
        "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
        "\n",
        "print('The categorical variables are :', categorical)"
      ],
      "metadata": {
        "id": "u4FQUJtOwsQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view the categorical variables\n",
        "\n",
        "data_df[categorical].head()\n"
      ],
      "metadata": {
        "id": "eXuvaSLDxCIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   ada informasi variabel \"term\" yang menjelaskan tentang jangka waktu pelunasan peminjaman\n",
        "*   ada 9 variabel categorical yang diantaranya term, grade, sub_grade, emp_length, home_ownership, verification_status, loan_status, purpose, last_credit_pull_d\n",
        "*   terdapat 3 variabel categorical binary: home_ownership, verification_status, loan_status\n",
        "*   loan_status adalah variabel target/dependen\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R30CKldaxjE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore problems within categorical variables"
      ],
      "metadata": {
        "id": "_W_JZpd-4bYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   missing values in categorical variabel\n",
        "\n"
      ],
      "metadata": {
        "id": "h-q6SkNU4xgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values in categorical variables\n",
        "\n",
        "data_df[categorical].isnull().sum()\n"
      ],
      "metadata": {
        "id": "V5DhXJGh4Z9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print categorical variables containing missing values\n",
        "\n",
        "cat1 = [var for var in categorical if data_df[var].isnull().sum()!=0]\n",
        "\n",
        "print(data_df[cat1].isnull().sum())"
      ],
      "metadata": {
        "id": "-_Gab4Nv5CMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view frequency of categorical variables\n",
        "\n",
        "for var in categorical:\n",
        "\n",
        "    print(data_df[var].value_counts())"
      ],
      "metadata": {
        "id": "xzAhzpu75QGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view frequency distribution of categorical variables\n",
        "\n",
        "for var in categorical:\n",
        "\n",
        "    print(data_df[var].value_counts()/float(len(data_df))) # Change np.float to float"
      ],
      "metadata": {
        "id": "IQe3ydmH5198"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for cardinality in categorical variables\n",
        "\n",
        "for var in categorical:\n",
        "\n",
        "    print(var, ' contains ', len(data_df[var].unique()), ' labels')"
      ],
      "metadata": {
        "id": "E4grDKbZ6uXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find numerical variables\n",
        "\n",
        "numerical = [var for var in data_df.columns if data_df[var].dtype!='O']\n",
        "\n",
        "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
        "\n",
        "print('The numerical variables are :', numerical)"
      ],
      "metadata": {
        "id": "eEY1Lj7_0u7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print numerical variables containing missing values\n",
        "\n",
        "cat1 = [var for var in numerical if data_df[var].isnull().sum()!=0]\n",
        "\n",
        "print(data_df[cat1].isnull().sum())"
      ],
      "metadata": {
        "id": "Cr2Q4lbU1zLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Misalkan 'df' adalah DataFrame yang memuat dataset Anda\n",
        "sns.histplot(data_df['loan_amnt'], bins=30, kde=True)\n",
        "plt.title('Distribusi Jumlah Pinjaman')\n",
        "plt.xlabel('Jumlah Pinjaman')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LTVkh0D_7RPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Sampling data untuk mengurangi kepadatan (contoh: ambil 20% dari total data)\n",
        "data_df_sampled = data_df.sample(frac=0.2, random_state=42)\n",
        "\n",
        "# Membuat scatter plot dengan pengaturan yang lebih mudah dibaca\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='loan_amnt',\n",
        "                y='int_rate',\n",
        "                data=data_df_sampled,\n",
        "                alpha=0.5,           # Transparansi untuk mengurangi overlap\n",
        "                s=30,                # Ukuran marker lebih kecil\n",
        "                color='blue')         # Warna yang kontras dan mudah dibaca\n",
        "\n",
        "# Menambah garis tren untuk menunjukkan pola umum\n",
        "sns.regplot(x='loan_amnt',\n",
        "            y='int_rate',\n",
        "            data=data_df_sampled,\n",
        "            scatter=False,\n",
        "            color='red',\n",
        "            line_kws={'linewidth': 2})\n",
        "\n",
        "# Menambah judul dan label\n",
        "plt.title('Hubungan antara Jumlah Pinjaman dan Tingkat Bunga')\n",
        "plt.xlabel('Jumlah Pinjaman')\n",
        "plt.ylabel('Tingkat Bunga (%)')\n",
        "\n",
        "# Menampilkan plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B_FZ1ca9Dbjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x='loan_status', y='loan_amnt', data=data_df)\n",
        "plt.title('Distribusi Jumlah Pinjaman berdasarkan Status Pinjaman')\n",
        "plt.xlabel('Status Pinjaman')\n",
        "plt.ylabel('Jumlah Pinjaman')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LdCvAt7KFeDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='int_rate', y='purpose', data=data_df, estimator=np.mean)\n",
        "plt.title('Rata-rata Tingkat Bunga per Tujuan Pinjaman')\n",
        "plt.xlabel('Rata-rata Tingkat Bunga (%)')\n",
        "plt.ylabel('Tujuan Pinjaman')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gas0_yoLJ_F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data_df[['loan_amnt', 'int_rate', 'annual_inc', 'dti']])\n",
        "plt.suptitle('Pair Plot Variabel Utama', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qdv5Xzn9Ljn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data_df['dti'], bins=30, kde=True)\n",
        "plt.title('Distribution of Debt-to-Income Ratio (DTI)')\n",
        "plt.xlabel('Debt-to-Income Ratio')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O2SiFz38N6_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(y='purpose', data=data_df, order=data_df['purpose'].value_counts().index)\n",
        "plt.title('Number of Borrowers by Loan Purpose')\n",
        "plt.xlabel('Number of Borrowers')\n",
        "plt.ylabel('Loan Purpose')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LlA8T9d9PSwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='verification_status', data=data_df)\n",
        "plt.title('Number of Borrowers by Verification Status')\n",
        "plt.xlabel('Verification Status')\n",
        "plt.ylabel('Number of Borrowers')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tPRP9wg3Y5sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation"
      ],
      "metadata": {
        "id": "tAFtBA0Blpq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = data_df.dropna(axis=1, how='all')"
      ],
      "metadata": {
        "id": "-Pory4canQqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "spB-r7mjnuPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.drop(['application_type'], axis=1, inplace=True)\n",
        "data_df.drop(['zip_code'], axis=1, inplace=True)\n",
        "data_df.drop(['desc'], axis=1, inplace=True)\n",
        "data_df.drop(['title'], axis=1, inplace=True)\n",
        "data_df.drop(['pymnt_plan'], axis=1, inplace=True)\n",
        "data_df.drop(['member_id'], axis=1, inplace=True)\n",
        "data_df.drop(['id'], axis=1, inplace=True)\n",
        "data_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "data_df.drop(['url'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "n_4xijw3nzCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "0UqOSjh6p8Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "vwY1ciEm8V82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['issue_d'].dtypes\n",
        "data_df['last_pymnt_d'].dtypes\n",
        "data_df['next_pymnt_d'].dtypes\n",
        "data_df['last_credit_pull_d'].dtypes\n",
        "data_df['earliest_cr_line'].dtypes"
      ],
      "metadata": {
        "id": "JKH1ukxZ8Y6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengonversi ke datetime\n",
        "\n",
        "data_df['issue_d'] = pd.to_datetime(data_df['issue_d'], errors='coerce', format='%b-%y')\n",
        "data_df['last_pymnt_d'] = pd.to_datetime(data_df['last_pymnt_d'], errors='coerce', format='%b-%y')\n",
        "data_df['next_pymnt_d'] = pd.to_datetime(data_df['next_pymnt_d'], errors='coerce', format='%b-%y')\n",
        "data_df['last_credit_pull_d'] = pd.to_datetime(data_df['last_credit_pull_d'], errors='coerce', format='%b-%y')\n",
        "data_df['earliest_cr_line'] = pd.to_datetime(data_df['earliest_cr_line'], errors='coerce', format='%b-%y')\n"
      ],
      "metadata": {
        "id": "Su-Dyapd8n1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengekstrak bulan dan tahun\n",
        "\n",
        "data_df['issue_d_month'] = data_df['issue_d'].dt.month\n",
        "data_df['last_pymnt_d_month'] = data_df['last_pymnt_d'].dt.month\n",
        "data_df['next_pymnt_d_month'] = data_df['next_pymnt_d'].dt.month\n",
        "data_df['last_credit_pull_d_month'] = data_df['last_credit_pull_d'].dt.month\n",
        "data_df['earliest_cr_line_month'] = data_df['earliest_cr_line'].dt.month\n"
      ],
      "metadata": {
        "id": "xVepkuy-9iqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Daftar kolom yang ingin ditampilkan\n",
        "columns_to_display = ['issue_d_month', 'last_pymnt_d_month', 'next_pymnt_d_month', 'last_credit_pull_d_month', 'earliest_cr_line_month']"
      ],
      "metadata": {
        "id": "F0rT180wkPEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan hanya kolom yang ditentukan\n",
        "selected_data = data_df[columns_to_display]\n",
        "\n",
        "# Tampilkan hasil\n",
        "print(selected_data)"
      ],
      "metadata": {
        "id": "nC5PcxGcuatv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "reA_xXT4ulU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labelling Variable Target"
      ],
      "metadata": {
        "id": "QHmnJppSwsw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['loan_status'].value_counts()"
      ],
      "metadata": {
        "id": "hZyqRVDKvTbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Membuat DataFrame contoh\n",
        "data = {\n",
        "    'loan_status': [\n",
        "        'Current',\n",
        "        'Fully Paid',\n",
        "        'Charged Off',\n",
        "        'Late (31-120 days)',\n",
        "        'In Grace Period',\n",
        "        'Does not meet the credit policy. Status:Fully Paid',\n",
        "        'Late (16-30 days)',\n",
        "        'Default',\n",
        "        'Does not meet the credit policy. Status:Charged Off'\n",
        "    ],\n",
        "    'count': [224226, 184739, 42475, 6900, 3146, 1988, 1218, 832, 761]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "GAhbhBtUwf0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "DsxYlH_8gyL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['loan_status'].tail()"
      ],
      "metadata": {
        "id": "mDeQvnGHM6pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mendefinisikan mapping untuk mengganti status\n",
        "\n",
        "# label_encoder = LabelEncoder() # This line is no longer needed\n",
        "status_mapping = {\n",
        "    'Current': 'good',\n",
        "    'Fully Paid': 'good',\n",
        "    'Charged Off': 'bad',\n",
        "    'Late (31-120 days)': 'bad',\n",
        "    'In Grace Period': 'bad',\n",
        "    'Does not meet the credit policy. Status:Fully Paid': 'good',\n",
        "    'Late (16-30 days)': 'bad',\n",
        "    'Default': 'bad',\n",
        "    'Does not meet the credit policy. Status:Charged Off': 'bad'\n",
        "}\n",
        "\n",
        "# Mengganti status pinjaman di data_df, bukan di df\n",
        "data_df['loan_status'] = data_df['loan_status'].map(status_mapping)\n",
        "\n",
        "# Tampilkan hasil\n",
        "print(data_df['loan_status'])"
      ],
      "metadata": {
        "id": "10-KoQIgwm5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['loan_status'].head()"
      ],
      "metadata": {
        "id": "3GUNR6BZygJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df"
      ],
      "metadata": {
        "id": "y1CBAYT3yyCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering"
      ],
      "metadata": {
        "id": "cVBstMQsb4oU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ordinal encoder"
      ],
      "metadata": {
        "id": "LQaLSVgIiE5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['term'].value_counts()"
      ],
      "metadata": {
        "id": "IqKoHzxclI_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['grade'].value_counts()\n"
      ],
      "metadata": {
        "id": "z3iGQ2D1mEc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['sub_grade'].value_counts()\n"
      ],
      "metadata": {
        "id": "LhyOhzgumHz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['emp_length'].value_counts()\n"
      ],
      "metadata": {
        "id": "MvUfGgMImKLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['verification_status'].value_counts()"
      ],
      "metadata": {
        "id": "i0oQjGCMmMd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder"
      ],
      "metadata": {
        "id": "WhWeCE0FFEIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat DataFrame contoh\n",
        "data = {\n",
        "    'term': ['36 months', '60 months', '36 months', '60 months', '36 months',\n",
        "             '60 months', '36 months', '60 months', '36 months', '60 months',\n",
        "             '36 months', '60 months', '36 months', '60 months', '36 months',\n",
        "             '60 months', '36 months', '60 months', '36 months', '60 months',\n",
        "             '36 months', '60 months', '36 months', '60 months', '36 months',\n",
        "             '60 months', '36 months', '60 months', '36 months', '60 months',\n",
        "             '36 months', '60 months', '36 months', '60 months', '36 months'],\n",
        "    'grade': ['B', 'C', 'D', 'A', 'E', 'F', 'G', 'B', 'C', 'D',\n",
        "              'A', 'E', 'F', 'G', 'B', 'C', 'D', 'A', 'E', 'F',\n",
        "              'G', 'B', 'C', 'D', 'A', 'E', 'F', 'G', 'B', 'C',\n",
        "              'D', 'A', 'E', 'F', 'G'],\n",
        "    'sub_grade': ['B3', 'B4', 'C1', 'C2', 'B2', 'C3', 'B5', 'B3', 'B4', 'C1',\n",
        "                  'C2', 'B2', 'C3', 'B5', 'B3', 'B4', 'C1', 'C2', 'B2', 'C3',\n",
        "                  'B5', 'B3', 'B4', 'C1', 'C2', 'B2', 'C3', 'B5', 'B3', 'B4',\n",
        "                  'C1', 'C2', 'B2', 'C3', 'B5'],\n",
        "    'emp_length': ['10+ years', '2 years', '3 years', '< 1 year', '5 years',\n",
        "                   '1 year', '4 years', '10+ years', '2 years', '3 years',\n",
        "                   '< 1 year', '10+ years', '2 years', '3 years', '< 1 year',\n",
        "                   '5 years', '1 year', '4 years', '10+ years', '2 years',\n",
        "                   '3 years', '< 1 year', '10+ years', '2 years', '3 years',\n",
        "                   '< 1 year', '5 years', '1 year', '4 years', '10+ years',\n",
        "                   '2 years', '3 years', '< 1 year', '5 years', '1 year'],\n",
        "    'verification_status': ['Verified', 'Not Verified', 'Verified', 'Verified',\n",
        "                            'Not Verified', 'Verified', 'Verified', 'Not Verified',\n",
        "                            'Verified', 'Verified', 'Not Verified', 'Verified',\n",
        "                            'Verified', 'Not Verified', 'Verified', 'Verified',\n",
        "                            'Not Verified', 'Verified', 'Verified', 'Not Verified',\n",
        "                            'Verified', 'Verified', 'Not Verified', 'Verified',\n",
        "                            'Verified', 'Not Verified', 'Verified', 'Verified',\n",
        "                            'Not Verified', 'Verified', 'Verified', 'Not Verified',\n",
        "                            'Verified', 'Verified', 'Not Verified']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Menampilkan DataFrame sebelum encoding\n",
        "print(\"DataFrame Sebelum Encoding:\")\n",
        "print(df)\n",
        "\n",
        "# 1. Menggunakan OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "df['term_ordinal'] = ordinal_encoder.fit_transform(df[['term']])\n",
        "df['grade_ordinal'] = ordinal_encoder.fit_transform(df[['grade']])\n",
        "df['sub_grade_ordinal'] = ordinal_encoder.fit_transform(df[['sub_grade']])\n",
        "df['emp_length_ordinal'] = ordinal_encoder.fit_transform(df[['emp_length']])\n",
        "df['verification_status_ordinal'] = ordinal_encoder.fit_transform(df[['verification_status']])\n",
        "\n",
        "print(\"\\nSetelah Ordinal Encoding:\")\n",
        "print(df[['term', 'term_ordinal']])\n",
        "print(df[['grade', 'grade_ordinal']])\n",
        "print(df[['sub_grade', 'sub_grade_ordinal']])\n",
        "print(df[['emp_length', 'emp_length_ordinal']])\n",
        "print(df[['verification_status', 'verification_status_ordinal']])\n"
      ],
      "metadata": {
        "id": "1EBUQfCpFRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assuming ordinal_encoder is already fitted as in your previous cells\n",
        "# and data_df is your original DataFrame\n",
        "\n",
        "data_df['term_ordinal'] = ordinal_encoder.fit_transform(data_df[['term']])\n",
        "data_df['grade_ordinal'] = ordinal_encoder.fit_transform(data_df[['grade']])\n",
        "data_df['sub_grade_ordinal'] = ordinal_encoder.fit_transform(data_df[['sub_grade']])\n",
        "data_df['emp_length_ordinal'] = ordinal_encoder.fit_transform(data_df[['emp_length']])\n",
        "data_df['verification_status_ordinal'] = ordinal_encoder.fit_transform(data_df[['verification_status']])\n",
        "\n",
        "# Now you have new columns with ordinal encodings in your original DataFrame"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nk3mMylwHGSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "ADe2FiPWHKDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.drop(['term'], axis=1, inplace=True)\n",
        "data_df.drop(['grade'], axis=1, inplace=True)\n",
        "data_df.drop(['sub_grade'], axis=1, inplace=True)\n",
        "data_df.drop(['emp_length'], axis=1, inplace=True)\n",
        "data_df.drop(['verification_status'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "KzXV66t9HsgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['home_ownership'].value_counts()"
      ],
      "metadata": {
        "id": "UVwXK1gqJmzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['purpose'].value_counts()\n"
      ],
      "metadata": {
        "id": "KbhCMcMIJqc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['addr_state'].value_counts()\n"
      ],
      "metadata": {
        "id": "BxvErkOxJtKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "tIPSAsFHJO9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat DataFrame contoh\n",
        "data = {\n",
        "    'home_ownership': ['MORTGAGE', 'RENT', 'OWN', 'OTHER', 'NONE', 'ANY'],\n",
        "    'purpose': ['debt_consolidation', 'credit_card', 'home_improvement',\n",
        "                'other', 'major_purchase', 'small_business', 'car', 'medical',\n",
        "                'moving', 'vacation', 'wedding', 'house', 'educational',  'renewable_energy'],\n",
        "    'addr_state': ['CA', 'NY', 'TX', 'FL', 'IL', 'NJ', 'PA', 'OH', 'GA', 'VA',\n",
        "                  'NC', 'MI', 'MA', 'MD', 'AZ', 'WA', 'CO', 'MN', 'MO', 'CT',\n",
        "                  'IN', 'NV', 'TN', 'OR', 'WI', 'AL', 'SC', 'LA', 'KY', 'KS',\n",
        "                  'OK', 'AR', 'UT', 'NM', 'HI', 'WV', 'NH', 'RI', 'DC', 'MT',\n",
        "                  'DE', 'AK', 'MS', 'WY', 'SD', 'VT', 'IA', 'NE', 'ID', 'ME'],\n",
        "    'initial_list_status': ['f', 'w'],\n",
        "\n",
        "}\n",
        "\n",
        "# Find the maximum length among all lists\n",
        "max_len = max(len(value) for value in data.values())\n",
        "\n",
        "# Pad shorter lists with None to match the maximum length\n",
        "padded_data = {\n",
        "    key: value + [None] * (max_len - len(value))\n",
        "    for key, value in data.items()\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(padded_data)\n",
        "# Menampilkan DataFrame sebelum encoding\n",
        "print(\"DataFrame Sebelum Encoding:\")\n",
        "print(df)\n",
        "\n",
        "# Menggunakan LabelEncoder untuk kolom 'home_ownership'\n",
        "label_encoder_home_ownership = LabelEncoder()\n",
        "df['home_ownership_label'] = label_encoder_home_ownership.fit_transform(df['home_ownership'])\n",
        "\n",
        "# Menggunakan LabelEncoder untuk kolom 'purpose'\n",
        "label_encoder_purpose = LabelEncoder()\n",
        "df['purpose_label'] = label_encoder_purpose.fit_transform(df['purpose'])\n",
        "\n",
        "# Menggunakan LabelEncoder untuk kolom 'addr_state'\n",
        "label_encoder_addr_state = LabelEncoder()\n",
        "df['addr_state_label'] = label_encoder_addr_state.fit_transform(df['addr_state'])\n",
        "\n",
        "# Menggunakan LabelEncoder untuk kolom 'initial_list_status'\n",
        "label_encoder_initial_list_status = LabelEncoder()\n",
        "df['initial_list_status_label'] = label_encoder_initial_list_status.fit_transform(df['initial_list_status'])\n",
        "# # 1. Menggunakan LabelEncoder\n",
        "# label_encoder = LabelEncoder()\n",
        "# df['home_ownership_label'] = label_encoder.fit_transform(df[['home_ownership']])\n",
        "# df['purpose_label'] = label_encoder.fit_transform(df[['purpose']])\n",
        "# df['addr_state_label'] = label_encoder.fit_transform(df[['addr_state']])\n",
        "# df['initial_list_status_label'] = label_encoder.fit_transform(df[['initial_list_status']])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hr9uocOzJWlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSetelah Label Encoder:\")\n",
        "print(df['home_ownership_label'])\n",
        "print(df['purpose_label'])\n",
        "print(df['addr_state_label'])\n",
        "print(df['initial_list_status_label'])"
      ],
      "metadata": {
        "id": "kuuwdSHPRUml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "cOHdezzcSMz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "data_df['home_ownership_label'] = label_encoder.fit_transform(data_df[['home_ownership']])\n",
        "data_df['purpose_label'] = label_encoder.fit_transform(data_df[['purpose']])\n",
        "data_df['addr_state_label'] = label_encoder.fit_transform(data_df[['addr_state']])\n",
        "data_df['initial_list_status_label'] = label_encoder.fit_transform(data_df[['initial_list_status']])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mKhZG3PbDpct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "PZ_IeOWjko8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.drop(['home_ownership'], axis=1, inplace=True)\n",
        "data_df.drop(['purpose'], axis=1, inplace=True)\n",
        "data_df.drop(['addr_state'], axis=1, inplace=True)\n",
        "data_df.drop(['initial_list_status'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "3DbFLc14SQ3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "sdSnQpjdSiYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.drop(['last_pymnt_d'], axis=1, inplace=True)\n",
        "data_df.drop(['next_pymnt_d'], axis=1, inplace=True)\n",
        "data_df.drop(['last_credit_pull_d'], axis=1, inplace=True)\n",
        "data_df.drop(['earliest_cr_line'], axis=1, inplace=True)\n",
        "data_df.drop(['issue_d'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "XSRqOW-JUSuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find numerical variables\n",
        "\n",
        "numerical = [var for var in data_df.columns if data_df[var].dtype!='O']\n",
        "\n",
        "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
        "\n",
        "print('The numerical variables are :', numerical)"
      ],
      "metadata": {
        "id": "btBWzsHK2JyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[numerical].head()"
      ],
      "metadata": {
        "id": "vRZrf-7v2ihH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[numerical].tail()"
      ],
      "metadata": {
        "id": "_vt6E6Mm3RQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[numerical].isnull().sum()"
      ],
      "metadata": {
        "id": "0BTXM8Ci3kYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "ttXRYmnUa_2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df"
      ],
      "metadata": {
        "id": "ko6rRj_SbF8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.isnull().sum()"
      ],
      "metadata": {
        "id": "UEcUI6Vjb2-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with impute data"
      ],
      "metadata": {
        "id": "g-g5G025bwZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_df['emp_title'].fillna(data_df['emp_title'].mean(), inplace=True)\n",
        "data_df['delinq_2yrs'].fillna(data_df['delinq_2yrs'].mean(), inplace=True)\n",
        "data_df['inq_last_6mths'].fillna(data_df['inq_last_6mths'].mean(), inplace=True)\n",
        "data_df['mths_since_last_delinq'].fillna(data_df['mths_since_last_delinq'].mean(), inplace=True)\n",
        "data_df['mths_since_last_record'].fillna(data_df['mths_since_last_record'].mean(), inplace=True)\n",
        "data_df['open_acc'].fillna(data_df['open_acc'].mean(), inplace=True)\n",
        "data_df['pub_rec'].fillna(data_df['pub_rec'].mean(), inplace=True)\n",
        "data_df['revol_util'].fillna(data_df['revol_util'].mean(), inplace=True)\n",
        "data_df['total_acc'].fillna(data_df['total_acc'].mean(), inplace=True)\n",
        "data_df['collections_12_mths_ex_med'].fillna(data_df['collections_12_mths_ex_med'].mean(), inplace=True)\n",
        "data_df['mths_since_last_major_derog'].fillna(data_df['mths_since_last_major_derog'].mean(), inplace=True)\n",
        "data_df['acc_now_delinq'].fillna(data_df['acc_now_delinq'].mean(), inplace=True)\n",
        "data_df['tot_coll_amt'].fillna(data_df['tot_coll_amt'].mean(), inplace=True)\n",
        "data_df['tot_cur_bal'].fillna(data_df['tot_cur_bal'].mean(), inplace=True)\n",
        "data_df['total_rev_hi_lim'].fillna(data_df['total_rev_hi_lim'].mean(), inplace=True)\n",
        "data_df['last_pymnt_d_month'].fillna(data_df['last_pymnt_d_month'].mean(), inplace=True)\n",
        "data_df['next_pymnt_d_month'].fillna(data_df['next_pymnt_d_month'].mean(), inplace=True)\n",
        "data_df['last_credit_pull_d_month'].fillna(data_df['last_credit_pull_d_month'].mean(), inplace=True)\n",
        "data_df['earliest_cr_line_month'].fillna(data_df['earliest_cr_line_month'].mean(), inplace=True)\n",
        "data_df['emp_length_ordinal'].fillna(data_df['emp_length_ordinal'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "YjXoJXWrbzXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['emp_title'].fillna(data_df['emp_title'].mode()[0], inplace=True)\n",
        "data_df['annual_inc'].fillna(data_df['annual_inc'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "id3-OL0BdfJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.isnull().sum()"
      ],
      "metadata": {
        "id": "pIUs2fEAdI_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "Qh3jrlzjd24F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memilih hanya kolom numerik\n",
        "numeric_df = data_df.select_dtypes(include=['number'])\n",
        "\n",
        "# Menghitung matriks korelasi\n",
        "correlation_matrix = numeric_df.corr()\n",
        "\n",
        "# Menampilkan heatmap\n",
        "plt.figure(figsize=(44, 33))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True)\n",
        "plt.title('Heatmap Korelasi Variabel Numerik')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5QQXm27JgzhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view summary statistics in numerical variables\n",
        "\n",
        "print(round(data_df.describe()),4)"
      ],
      "metadata": {
        "id": "3u_wouqSi0iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw boxplots to visualize outliers\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 1)\n",
        "fig = data_df.boxplot(column='loan_amnt')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('loan_amnt')\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 2)\n",
        "fig = data_df.boxplot(column='funded_amnt')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('funded_amnt')\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 3)\n",
        "fig = data_df.boxplot(column='funded_amnt_inv')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('funded_amnt_inv')\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 4)\n",
        "fig = data_df.boxplot(column='annual_inc')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('annual_inc')\n",
        "\n"
      ],
      "metadata": {
        "id": "M5aDOsPL5AXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot histogram to check distribution\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 1)\n",
        "fig = data_df.annual_inc.hist(bins=10)\n",
        "fig.set_xlabel('loan_amnt')\n",
        "fig.set_ylabel('loan_status')\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 2)\n",
        "fig = data_df.delinq_2yrs.hist(bins=10)\n",
        "fig.set_xlabel('funded_amnt')\n",
        "fig.set_ylabel('loan_status')\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 3)\n",
        "fig = data_df.revol_bal.hist(bins=10)\n",
        "fig.set_xlabel('funded_amnt_inv')\n",
        "fig.set_ylabel('loan_status')\n",
        "\n",
        "\n",
        "plt.subplot(4, 2, 4)\n",
        "fig = data_df.revol_util.hist(bins=10)\n",
        "fig.set_xlabel('annual_inc')\n",
        "fig.set_ylabel('loan_status')"
      ],
      "metadata": {
        "id": "tJgUoy_c-CbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find outliers for loan_amnt variable\n",
        "\n",
        "IQR = data_df.loan_amnt.quantile(0.75) - data_df.loan_amnt.quantile(0.25)\n",
        "Lower_fence = data_df.loan_amnt.quantile(0.25) - (IQR * 3)\n",
        "Upper_fence = data_df.loan_amnt.quantile(0.75) + (IQR * 3)\n",
        "print('loan_amnt outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))"
      ],
      "metadata": {
        "id": "GUob3PMz_oPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find outliers for funded_amnt variable\n",
        "\n",
        "IQR = data_df.funded_amnt.quantile(0.75) - data_df.funded_amnt.quantile(0.25)\n",
        "Lower_fence = data_df.funded_amnt.quantile(0.25) - (IQR * 3)\n",
        "Upper_fence = data_df.funded_amnt.quantile(0.75) + (IQR * 3)\n",
        "print('funded_amnt outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))"
      ],
      "metadata": {
        "id": "UIsvBa-6AQ_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find outliers for funded_amnt_inv variable\n",
        "\n",
        "IQR = data_df.funded_amnt_inv.quantile(0.75) - data_df.funded_amnt_inv.quantile(0.25)\n",
        "Lower_fence = data_df.funded_amnt_inv.quantile(0.25) - (IQR * 3)\n",
        "Upper_fence = data_df.funded_amnt_inv.quantile(0.75) + (IQR * 3)\n",
        "print('funded_amnt_inv outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))"
      ],
      "metadata": {
        "id": "AdCd4ktqAV0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find outliers for revol_util variable\n",
        "\n",
        "IQR = data_df.revol_util.quantile(0.75) - data_df.revol_util.quantile(0.25)\n",
        "Lower_fence = data_df.revol_util.quantile(0.25) - (IQR * 3)\n",
        "Upper_fence = data_df.revol_util.quantile(0.75) + (IQR * 3)\n",
        "print('revol_util outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))"
      ],
      "metadata": {
        "id": "jYcltT9iAdRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find outliers for annual_inc variable\n",
        "\n",
        "IQR = data_df.annual_inc.quantile(0.75) - data_df.annual_inc.quantile(0.25)\n",
        "Lower_fence = data_df.annual_inc.quantile(0.25) - (IQR * 3)\n",
        "Upper_fence = data_df.annual_inc.quantile(0.75) + (IQR * 3)\n",
        "print('annual_inc outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))"
      ],
      "metadata": {
        "id": "-xsnSP4hAi_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "one hot encoding"
      ],
      "metadata": {
        "id": "LUWav-iIVW5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['emp_title'].value_counts()"
      ],
      "metadata": {
        "id": "y6EaME0g9FNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['loan_status'].value_counts()"
      ],
      "metadata": {
        "id": "7xie8_JMUp3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_df = pd.concat([data_df, Emp_title, Loan_status], axis=1)"
      ],
      "metadata": {
        "id": "DHIu19YVxmz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "Cm-WY-ekVrpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "work with emp_title and loan_status variable"
      ],
      "metadata": {
        "id": "LM_hVze2p09i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat DataFrame contoh\n",
        "data = {\n",
        "    'loan_status': ['good', 'bad']\n",
        "}\n",
        "\n",
        "# Find the maximum length among all lists\n",
        "max_len = max(len(value) for value in data.values())\n",
        "\n",
        "# Pad shorter lists with None to match the maximum length\n",
        "padded_data = {\n",
        "    key: value + [None] * (max_len - len(value))\n",
        "    for key, value in data.items()\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(padded_data)\n",
        "# Menampilkan DataFrame sebelum encoding\n",
        "print(\"DataFrame Sebelum Encoding:\")\n",
        "print(df)\n",
        "\n",
        "# Menggunakan LabelEncoder untuk kolom 'home_ownership'\n",
        "label_encoder_loan_status = LabelEncoder()\n",
        "df['loan_status_label'] = label_encoder_loan_status.fit_transform(df['loan_status'])\n"
      ],
      "metadata": {
        "id": "e5nT2Kr7mDPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "print(\"\\nSetelah Label Encoder:\")\n",
        "print(df['loan_status_label'])"
      ],
      "metadata": {
        "id": "QJUXGTyvoYtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "data_df['loan_status_label'] = label_encoder_loan_status.fit_transform(data_df['loan_status'])\n"
      ],
      "metadata": {
        "id": "1NUE5dDMora0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "Q-d1YRYKpwPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.drop(['emp_title'], axis=1, inplace=True)\n",
        "data_df.drop(['loan_status'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "TdNhvcz33C6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "nWGKtX4c38_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_df.drop(['loan_status_label'], axis=1)\n",
        "\n",
        "y = data_df['loan_status_label']"
      ],
      "metadata": {
        "id": "2HlAyc09BjAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split X and y into training and testing sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "4JiZ07ZVBpsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the shape of X_train and X_test\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "EjvXJzD9BskB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check data types in X_train\n",
        "\n",
        "X_train.dtypes"
      ],
      "metadata": {
        "id": "Nr540WSVBv9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display categorical variables\n",
        "\n",
        "categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n",
        "\n",
        "categorical"
      ],
      "metadata": {
        "id": "vCdPsyZgB3oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display numerical variables\n",
        "\n",
        "numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n",
        "\n",
        "numerical"
      ],
      "metadata": {
        "id": "ceJlEIdfB7gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scalling"
      ],
      "metadata": {
        "id": "VM6aFr8JTfQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe()"
      ],
      "metadata": {
        "id": "QfTvMpFEThOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.DataFrame(X_train, columns=X_train.columns) # Convert X_train back to DataFrame"
      ],
      "metadata": {
        "id": "DUFQDxaWuBA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = X_train.columns\n"
      ],
      "metadata": {
        "id": "rtTLnCTfTjdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.columns)\n",
        "print(X_test.columns)"
      ],
      "metadata": {
        "id": "H7IxaltzTlzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)"
      ],
      "metadata": {
        "id": "24Tpx_8EToHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test[X_train.columns]"
      ],
      "metadata": {
        "id": "HPpK9lvcTXGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "Gcz2lWVE-Yk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df"
      ],
      "metadata": {
        "id": "KDccGUxHK4yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "iS7LyAccS6L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "WtTJSC_CToIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "0LEyhM4Xujsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train_scaled:\")\n",
        "print(X_train_scaled)\n"
      ],
      "metadata": {
        "id": "2jHv3ObxVvu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nX_test_scaled:\")\n",
        "print(X_test_scaled)"
      ],
      "metadata": {
        "id": "2SwuR0jbuv0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe()"
      ],
      "metadata": {
        "id": "PmSAV9aQuzZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "TMTid2a0u8XV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['loan_status_label'].value_counts()"
      ],
      "metadata": {
        "id": "4FAHAhBayh8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "CJJazfBU590Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.value_counts()"
      ],
      "metadata": {
        "id": "1CH5gVmS59q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "I4xGLVakypN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.head()"
      ],
      "metadata": {
        "id": "L6NzSWnRytQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_test = logreg.predict(X_test)\n",
        "print('Training set score: {:.4f}'.format(logreg.score(X_train, y_train)))\n",
        "print('Test set score: {:.4f}'.format(logreg.score(X_test, y_test)))\n",
        "\n",
        "# Instantiate the model\n",
        "logistic_regression_model = LogisticRegression(random_state=0)\n",
        "\n",
        "# Fit the model\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = logistic_regression_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Logistic Regression Classifier:\")\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print('Confusion matrix\\n\\n', cm)\n",
        "\n",
        "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
        "\n",
        "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
        "\n",
        "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
        "\n",
        "print('\\nFalse Negatives(FN) = ', cm[1,0])"
      ],
      "metadata": {
        "id": "SduaqPGJujxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred_test"
      ],
      "metadata": {
        "id": "sUWcx8YQulo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probability of getting output as 0 - no rain\n",
        "\n",
        "# logreg.predict_proba(X_test)[:,0]"
      ],
      "metadata": {
        "id": "EPaoNA_cu8F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probability of getting output as 1 - rain\n",
        "\n",
        "# logreg.predict_proba(X_test)[:,1]"
      ],
      "metadata": {
        "id": "2ttzHy70vAE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))"
      ],
      "metadata": {
        "id": "6xZerOsZvAh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred_train = logreg.predict(X_train)\n",
        "\n",
        "# y_pred_train"
      ],
      "metadata": {
        "id": "5ZYMhU-7vENt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the scores on training and test set\n",
        "\n"
      ],
      "metadata": {
        "id": "ILCKukTrvLyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the Logsitic Regression model with C=001\n",
        "\n",
        "# instantiate the model\n",
        "# logreg001 = LogisticRegression(C=0.01, solver='liblinear', random_state=0)\n",
        "\n",
        "\n",
        "# # fit the model\n",
        "# logreg001.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "p-tHdYdWvP2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the scores on training and test set\n",
        "\n",
        "# print('Training set score: {:.4f}'.format(logreg001.score(X_train, y_train)))\n",
        "\n",
        "# print('Test set score: {:.4f}'.format(logreg001.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "VTqy6Ca8vhx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check class distribution in test set\n",
        "\n",
        "# y_test.value_counts()"
      ],
      "metadata": {
        "id": "FEA4JinzvzLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check null accuracy score\n",
        "\n",
        "# null_accuracy = (82206/(82206+11051))\n",
        "\n",
        "# print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
      ],
      "metadata": {
        "id": "Q1bXyhnzv10N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3u3p0Ew-v5on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. random forest"
      ],
      "metadata": {
        "id": "2mHhM5N0yP_u"
      }
    },
    {
      "source": [
        "# train a random forest model on the training set\n",
        "# from sklearn.ensemble import RandomForestClassifier # Import RandomForestClassifier from sklearn.ensemble\n",
        "\n",
        "# # instantiate the model\n",
        "# random_forest = RandomForestClassifier(random_state=0) # Remove solver='liblinear'\n",
        "\n",
        "# # fit the model\n",
        "# random_forest.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NLFejrdm3To9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier()\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred= rf_clf.predict(X_test)\n",
        "print(\"Accuracy on Traing set: \",rf_clf.score(X_train,y_train))\n",
        "print(\"Accuracy on Testing set: \",rf_clf.score(X_test,y_test))\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Instantiate the model\n",
        "random_forest_model = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Fit the model\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Print the Confusion Matrix and slice it into four pieces\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print('Confusion matrix\\n\\n', cm)\n",
        "\n",
        "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
        "\n",
        "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
        "\n",
        "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
        "\n",
        "print('\\nFalse Negatives(FN) = ', cm[1,0])"
      ],
      "metadata": {
        "id": "gNFKrLu15D7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. XGBoost Classifier\n"
      ],
      "metadata": {
        "id": "es2DN197OfKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Instantiate the model\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Fit the model\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"XGBoost Classifier:\")\n",
        "print(\"Accuracy on Training set: \", xgb_clf.score(X_train, y_train))\n",
        "print(\"Accuracy on Testing set: \", xgb_clf.score(X_test, y_test))\n",
        "\n",
        "# Optional: Print classification report for more detailed evaluation\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# Print the Confusion Matrix and slice it into four pieces\n",
        "\n",
        "# Instantiate the model\n",
        "xgboost_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Fit the model\n",
        "xgboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_xgb = xgboost_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"XGBoost Classifier:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print('Confusion matrix\\n\\n', cm)\n",
        "\n",
        "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
        "\n",
        "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
        "\n",
        "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
        "\n",
        "print('\\nFalse Negatives(FN) = ', cm[1,0])"
      ],
      "metadata": {
        "id": "_wYtETC_OxN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. LightGBM"
      ],
      "metadata": {
        "id": "atEGTB1ROjsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Instantiate the model\n",
        "lgbm_clf = LGBMClassifier()\n",
        "\n",
        "# Fit the model\n",
        "lgbm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lgbm = lgbm_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"LightGBM Regressor:\")\n",
        "print(\"Accuracy on Training set: \", lgb_reg.score(X_train, y_train))\n",
        "print(\"Accuracy on Testing set: \", lgb_reg.score(X_test, y_test))\n",
        "\n",
        "# Instantiate the model\n",
        "lightgbm_model = LGBMClassifier()\n",
        "\n",
        "# Fit the model\n",
        "lightgbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lgb = lightgbm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"LightGBM Classifier:\")\n",
        "print(classification_report(y_test, y_pred_lgb))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print('Confusion matrix\\n\\n', cm)\n",
        "\n",
        "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
        "\n",
        "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
        "\n",
        "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
        "\n",
        "print('\\nFalse Negatives(FN) = ', cm[1,0])"
      ],
      "metadata": {
        "id": "cp214USzO1CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Decision Tree Classifier\n"
      ],
      "metadata": {
        "id": "fypwmT84OnoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Instantiate the model\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model\n",
        "dt_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Decision Tree Regressor:\")\n",
        "print(\"Accuracy on Training set: \", dt_clf.score(X_train, y_train))\n",
        "print(\"Accuracy on Testing set: \", dt_clf.score(X_test, y_test))\n",
        "\n",
        "# Instantiate the model\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "# Fit the model\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt = decision_tree_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Decision Tree Classifier:\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print('Confusion matrix\\n\\n', cm)\n",
        "\n",
        "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
        "\n",
        "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
        "\n",
        "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
        "\n",
        "print('\\nFalse Negatives(FN) = ', cm[1,0])"
      ],
      "metadata": {
        "id": "q4OA9u-dO2wg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}